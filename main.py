"""
Aplicaci√≥n principal de Streamlit - Dr. Sanal, sistema acad√©mico sin l√≠mites artificiales.
"""

import os
from datetime import datetime
from typing import List, Dict, Optional

import streamlit as st

from prompts import get_system_prompt
from openai_handler import (
    chat_with_sanal,
    generate_academic_work_phased,
    build_context_block,
)
from file_processor import prepare_context_from_files


# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Dr. Sanal - Profesor de Psicolog√≠a",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded",
)


# Estilos compactos y alto contraste
st.markdown(
    """
<style>
    .stApp { background-color: #0f0f0f; color: #f5f5f5; }
    .stChatMessage, .stChatMessage p, .stMarkdown { color: #f5f5f5 !important; }
    .bubble-user { background:#1f2a44; border-left:3px solid #4a9eff; padding:12px; border-radius:4px; margin-bottom:10px; color:#f5f5f5; }
    .bubble-assistant { background:#252525; border-left:3px solid #c41e3a; padding:12px; border-radius:4px; margin-bottom:10px; font-style: italic; color:#f5f5f5; }
    .attachment-pill { display:block; padding:8px 10px; margin:6px 0; background:#1f1f1f; border:1px solid #444; border-radius:10px; font-size:0.9em; color:#f5f5f5; }
    .small { font-size:0.85em; color:#cfcfcf; }
</style>
""",
    unsafe_allow_html=True,
)


# Estado de sesi√≥n
st.session_state.setdefault("chat_history", [])
st.session_state.setdefault("attachments", [])


# Verificar API key
if not os.getenv("OPENAI_API_KEY"):
    st.error("‚ö†Ô∏è ERROR: No se encontr√≥ OPENAI_API_KEY en las variables de entorno. Configura tu .env file")
    st.stop()


# Sidebar
with st.sidebar:
    st.markdown("### ‚öôÔ∏è Configuraci√≥n simple")
    language_choice = st.selectbox(
        "Idioma de salida",
        ["Autom√°tico", "Catal√†", "Castellano", "English"],
        help="Si eliges Autom√°tico, responde en el idioma del usuario.",
    )

    st.divider()
    st.markdown("### üóÇÔ∏è Adjuntos en contexto")
    uploaded_files = st.file_uploader(
        "Sube archivos (PDF, DOCX, TXT/MD, CSV/XLSX, JPG/PNG)",
        type=["pdf", "docx", "txt", "md", "csv", "xlsx", "xls", "jpg", "jpeg", "png", "webp"],
        accept_multiple_files=True,
    )
    col_up1, col_up2 = st.columns(2)
    if col_up1.button("A√±adir al contexto", use_container_width=True):
        if uploaded_files:
            with st.spinner("Procesando adjuntos..."):
                processed = prepare_context_from_files(uploaded_files)
                st.session_state.attachments.extend(processed)
                st.success(f"{len(processed)} adjunto(s) listos")
                st.rerun()
    if col_up2.button("Limpiar adjuntos", use_container_width=True):
        st.session_state.attachments = []
        st.rerun()

    if st.session_state.attachments:
        st.markdown(f"{len(st.session_state.attachments)} archivo(s):")
        for idx, att in enumerate(st.session_state.attachments, start=1):
            st.markdown(
                f"<span class='attachment-pill'>[{idx}] {att.get('name','archivo')} ‚Ä¢ {att.get('kind','?')}<br><span class='small'>{att.get('summary','(sin resumen)')[:200]}</span></span>",
                unsafe_allow_html=True,
            )
            if st.button("Quitar", key=f"rm_sidebar_{idx}", use_container_width=True):
                st.session_state.attachments.pop(idx - 1)
                st.rerun()

    st.divider()
    st.markdown("### üßπ Limpiar conversaci√≥n")
    if st.button("Reiniciar chat", use_container_width=True):
        st.session_state.chat_history = []
        st.session_state.attachments = []
        st.rerun()

    st.divider()
    st.markdown("### ‚ÑπÔ∏è Comandos r√°pidos")
    st.markdown(
        """
        - `/nota ...` pide calificaci√≥n 0-10.
        - `/generar ...` redacta un trabajo.
        - `/limpiar` reinicia chat y adjuntos.
        """
    )

    st.divider()
    st.markdown("### ‚ÑπÔ∏è Acerca de")
    st.markdown(
        """
        **Dr. Sanal** es un sistema de apoyo acad√©mico dise√±ado para:
        - An√°lisis cr√≠tico de trabajos (APA 7, metodolog√≠a)
        - An√°lisis estad√≠stico riguroso
        - Generaci√≥n de trabajos optimizados para m√°xima calidad
        - SIN l√≠mites artificiales de palabras ni notas objetivo
        """
    )


# Header
col1, col2 = st.columns([0.8, 0.2])
with col1:
    st.markdown("### üéì Dr. Sanal - Catedr√°tico de Psicolog√≠a")
    st.markdown("*Universitat Oberta de Catalunya - Departamento de Psicolog√≠a*")
with col2:
    st.markdown(f"üìÖ {datetime.now().strftime('%d/%m/%Y')}")

st.divider()


# Conversaci√≥n
st.markdown("### üí¨ Chat unificado")

for message in st.session_state.chat_history:
    with st.chat_message("user" if message["role"] == "user" else "assistant"):
        st.markdown(message["content"])


def handle_command(user_text: str, context_block: str, language_choice: str, attachments: List[Dict]) -> str:
    """Gestiona comandos especiales como /nota o /generar."""
    lower = user_text.lower()

    if lower.startswith("/limpiar"):
        st.session_state.chat_history = []
        st.session_state.attachments = []
        st.rerun()

    if lower.startswith("/nota"):
        question = user_text[len("/nota"):].strip() or "Califica el trabajo adjunto con nota 0-10 REAL basada en criterios UOC y justifica APA/metodolog√≠a/coherencia."
        system_prompt = get_system_prompt("analysis")
        control = f"Idioma: {language_choice}." if language_choice != "Autom√°tico" else ""
        content = f"{context_block}\n\n{control}\n\nInstrucci√≥n: {question}"
        # Guardar texto para contador de tokens
        st.session_state["last_prompt_text"] = f"SYSTEM:\n{get_system_prompt('analysis')}\n\nUSER:\n{content}"
        return chat_with_sanal(
            messages=[{"role": "user", "content": build_content_with_images(content, attachments)}],
            system_prompt=system_prompt,
            temperature=0.7,
            max_tokens=3000,
            context="analysis",
            complexity=0.7,
            force_model=None,
        )

    if lower.startswith("/generar"):
        rest = user_text[len("/generar"):].strip()
        topic = rest or "Trabajo acad√©mico solicitado"
        requirements = f"Usa los adjuntos como insumo. Detalles del usuario: {rest}"
        lang_hint = None if language_choice == "Autom√°tico" else ("ca" if language_choice == "Catal√†" else ("es" if language_choice == "Castellano" else "en"))

        work = generate_academic_work_phased(
            topic=topic,
            requirements=requirements,
            attachments=st.session_state.attachments,
            language_hint=lang_hint,
        )

        st.markdown("## üìÑ Trabajo Generado")
        st.markdown(work)
        st.divider()
        st.success("‚úì Trabajo generado con arquitectura por fases (estable)", icon="‚úì")
        return ""

    return ""


def looks_truncated(text: str) -> bool:
    """Heur√≠stica simple para detectar respuestas cortadas."""
    if not text or len(text) < 200:
        return False
    tail = text[-200:]
    # Incompleto si no termina con puntuaci√≥n fuerte
    ends_ok = any(text.strip().endswith(p) for p in [".", "!", "?", ")", "]", "\""])
    has_heading = any(h in tail.lower() for h in ["referencias", "bibliograf√≠a", "discusi√≥n", "resultados"])
    mid_word_cut = tail.endswith(" ") and not ends_ok
    return (not ends_ok) or mid_word_cut or has_heading


def continue_generation(previous_text: str, language_hint: Optional[str], attachments: List[Dict]) -> str:
    """Solicita continuaci√≥n del texto sin repetir contenido previo."""
    lang_line = f"Idioma: {language_hint}" if language_hint else "Idioma: autom√°tico"
    instruction = f"""
Contin√∫a el trabajo EXACTAMENTE desde donde se qued√≥.
No repitas contenido previo. Mant√©n estructura acad√©mica.
Si estaba en medio de una secci√≥n, compl√©tala y contin√∫a.
{lang_line}
"""
    return chat_with_sanal(
        messages=[{"role": "user", "content": build_content_with_images(previous_text + "\n\n" + instruction, attachments)}],
        system_prompt=get_system_prompt("generation"),
        temperature=0.7,
        max_tokens=8000,
        context="generation",
        complexity=0.8,
        force_model=None,
    )


def build_content_with_images(text_block: str, attachments: List[Dict]) -> List[Dict]:
    """Prepara contenido multimodal para OpenAI (texto + im√°genes base64)."""
    content: List[Dict] = [{"type": "text", "text": text_block}]
    for att in attachments:
        if att.get("kind") == "image" and att.get("base64"):
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{att['base64']}"
                }
            })
    return content


if user_input := st.chat_input("Escribe o usa comandos /nota, /generar"):
    user_message = {"role": "user", "content": user_input}
    context_block = build_context_block(st.session_state.attachments)
    control_block = f"Idioma seleccionado: {language_choice}."

    # Ejecutar comandos primero
    response = handle_command(user_input, context_block, language_choice, st.session_state.attachments)

    if not response:  # Chat normal
        system_prompt = get_system_prompt("chat")
        user_content = build_content_with_images(
            f"{context_block}\n\n{control_block}\n\nMensaje del estudiante: {user_input}",
            st.session_state.attachments,
        )
        messages_for_api = st.session_state.chat_history + [
            {
                "role": "user",
                "content": user_content,
            }
        ]

        # Guardar prompt para contador
        st.session_state["last_prompt_text"] = f"SYSTEM:\n{get_system_prompt('chat')}\n\nUSER:\n{context_block}\n\n{control_block}\n\nMensaje: {user_input}"

        response = chat_with_sanal(
            messages=messages_for_api,
            system_prompt=system_prompt,
            temperature=0.7,
            max_tokens=3000,
            context="chat",
            complexity=0.5,
            force_model=None,
        )

    st.session_state.chat_history.append(user_message)
    st.session_state.chat_history.append({"role": "assistant", "content": response})
    st.rerun()


# Footer
st.divider()
st.markdown(
    """
---
**Dr. Sanal** - Sistema de Apoyo Acad√©mico | UOC Psicolog√≠a  
*"La excelencia acad√©mica no es negociable"* - Dr. Sanal
"""
)
